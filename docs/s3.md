# MSI Second Tier ("tier 2") Storage & S3 Bucket Interface
Second tier storage is used for storing data and files that you don't need readily accessible on tier 1. Each user has 120 TB of space, which is why it is preferred to store data here as opposed to tier 1. 

For a comprehensive introduction to s3, see [this tutorial video](https://drive.google.com/drive/folders/1Oz3i5lbld5VmXGdhWagOMWYOIJmgrJA4) (recorded as part of the Data Processing Workshop hosted for undergraduate research assistants). It is encouraged to watch this tutorial prior to your first time using s3 commands. Note that after you run workbench commands, make sure to `module rm workbench` prior to doing any `s3cmd` commands.


## S3 Basics and Usage

### How S3cmd Works
It's important to know that S3 is not actually a filesystem, but an object store, and S3cmd is an API tool for object storage. As a result, the logic you are likely more familiar with when interacting with folder structures on the command line doesn't necessarily translate to s3cmd. S3 buckets don't actually have real directory structures, so when referencing a specific file or folder on S3, you provide what may *look* like a file path, but is actually a *key prefix* used to iterate through objects with matching substrings. 

For example, to list the contents of s3://luci-scratch/ABCC, it is important to include a trailing `/` (i.e. s3://luci-scratch/ABCC/). Without a trailing `/`, s3cmd will iterate through all objects containing `luci-scratch/ABCC*`, which does not include the contents of the folder itself:
```
$ s3cmd ls s3://luci-scratch/ABCC
                    DIR  s3://luci-scratch/ABCC/
```

When the trailing `/` is present, s3cmd iteratates through the bucket objects to list objects containing `luci-scratch/ABCC*/*` and return the following:
```
$ s3cmd ls s3://luci-scratch/ABCC/
                    DIR  s3://luci-scratch/ABCC/code/
                    DIR  s3://luci-scratch/ABCC/data/
2024-09-09 21:25 DIROBJ  s3://luci-scratch/ABCC/
2024-09-09 21:42   183M  s3://luci-scratch/ABCC/subjectlist.csv

```

Note that this logic also means that wildcard (`*`) doesn't work for s3 commands

### S3cmd Usage (see [s3cmd usage](https://s3tools.org/usage) for full usage)
**General**<br>
`s3info -u x500` - view your s3 credentials and tier2 disk usage 

`s3cmd du -H s3://bucket-name/` -check the size of a single bucket (when using commands like `du` or `ls` on large buckets, we recommend using [s5cmd client](#s5cmd-client)
 
`s3cmd mb s3://bucket-name/` - make a new bucket (*NOTE:* while not always the case, MSI currently requires users to follow AmazonS3 naming standards, e.g. names cannot include underscores or uppercase letters - see the [AmazonS3 bucket naming rules](https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucketnamingrules.html) for full details)

`s3cmd ls` - list all buckets you are owner on (buckets that you do not own, but have been given access to, will not appear in this list) 

**Moving data to and from s3 buckets**<br>
`s3cmd sync /path/to/file s3://bucket-name/` - sync a file to a bucket

`s3cmd sync s3://bucket-name/file /path/to/folder/output/` - sync a file from a bucket

`s3cmd sync s3://bucket-name/dir_name /destination/directory/path/ --recursive` - sync a folder from a bucket

`s3cmd sync dir_name s3://bucket-name/  --recursive` - sync a folder to a bucket (*NOTE:* if you only want to clear the prior contents of the directory that you are syncing to, include the `--delete-removed` flag to remove files that are present in the destination path but aren't present in the source path. *USE WITH CAUTION*)

If your job runs out of time before the data sync is complete, use `s3cmd sync --recursive --no-check-md5` to restart the sync process where it left off (even if the sync says complete)

*Or use `get`/`put`*:<br>
`s3cmd get s3://path/to/file /path/to/where/you/want/file/` - pull down single file

`s3cmd put path/to/file s3://path/to/where/you/want/file/` - place a single file into bucket

**Removing data**<br>
`s3cmd rm s3://bucket-name/file` - remove a file from a bucket

`s3cmd rb s3://bucket-name` - remove an entire bucket


## Updating S3 Policy & Bucket Access  

To gain access to a bucket, ask the bucket owner or others with access granted. If you're unsure of who already has access, consult the tier2 tab of CDNI's [tracking spreadsheet](https://docs.google.com/spreadsheets/d/1QpKYJQqhuxoQhErBscAEev9npsd1RgKS8KdCL6FiuEo/edit?usp=sharing) 

To give someone access to a bucket, MSI provides instruction [here](https://www.msi.umn.edu/support/faq/how-do-i-use-s3-buckets-share-data-tier-2-storage-other-users) on how to set up/modify an s3 policy, but the simplest method is to use the `set_s3policy` command as follows:  

First make sure the following line is included in your `.bashrc`: `export PATH=/home/dhp/public/storage/s3policy_bin/:$PATH`. If you have to add it, remember to close your current temrinal open a new terminal after saving your changes in order for your changes to take effect

Here's an example of how to update the s3 policy for a bucket to add user with x500 `newuserx500`:

Print list of current users with bucket access (*note that this script will not work if you don't already have bucket access*):
```
$ /home/faird/shared/code/internal/utilities/MSI-utilities/s3_get_x500/get_x500.sh s3://bucket_name/
  x500_1,x500_2
```

Copy list exactly, append the x500 for new user you wish to grant access to this list, include updated list as the second input to the following command (*NOTE:* do not include the s3:// prefix for the bucket name in the command):
```
$ set_s3policy x500_1,x500_2,newuserx500 bucket_name FULL
  s3://bucket_name/: Policy updated
```
**IMPORTANT: every time you set a policy you need to list EVERYONE that needs access. You CANNOT just list that one person's x500 or only that one person will have access to the bucket! (Along with the bucket owner)**

Finally, update the list of users with access to the bucket in tier2 tab of CDNI's [tracking spreadsheet](https://docs.google.com/spreadsheets/d/1QpKYJQqhuxoQhErBscAEev9npsd1RgKS8KdCL6FiuEo/edit?usp=sharing)


## Setting up a .s3cfg

79. Setting up a .s3cfg file: 

    - Open a new terminal in MSI and create a file named `.s3cfg-RANDOM` in your home directory

        * Can't just be `.s3cfg`, name it something identifiable 

    - Run `s3cmd -c ~/.s3cfg-RANDOM --configure` and provide your Access Key ID and Secret Access Key when prompted

    - You shouldn't have to make any changes to the file unless the s3 bucket is hosted by something different than *s3.amazonaws.com*  

    - Test your access through an `ls` command - the `s3cmd -c ~/.s3cfg-RANDOM` will have to be part of every command you run to access the buckets that require the credentials you provided.

## s3 access with Cyberduck 
Resource: [View your MSI S3 credentials (login required)](https://www.msi.umn.edu/content/s3-credentials) 

Read: [Cyberduck S3 documentation](https://docs.cyberduck.io/protocols/s3/)

[Cyberduck](https://cyberduck.io/) is a cloud storage browser for Windows and macOS devices, which can be used to access s3 storage from your local machine. To access MSI Tier 2 storage in Cyberduck, connect to *s3.msi.umn.edu* using your s3 credentials, which can be retrieved from the link at the top of this subsection or by running `s3info` in your MSI terminal. 

To log in for the first time, follow the following steps:

- Click "Open Connection" in the top left corner of the screen

- Select "Amazon s3" from the drop down menu

- Type in *s3.msi.umn.edu* for the server 

- Enter you access and secret keys 

- Click "Connect"

## s5cmd client

Read: [s5cmd GitHub](https://github.com/peak/s5cmd)

The **s5cmd** client is installed on MSI at `/home/faird/shared/code/external/utilities/s5cmd/s5cmd`.

**s5cmd** is an alternative S3 client that performs much faster on MSI than s3cmd for certain operations (roughly 10x-100x faster for `du` and `ls`). Its usage is similar to s3cmd, but does require the additional argument  `--endpoint-url https://s3.msi.umn.edu` so that commands are directed to MSI's S3 (Tier 2) and not AWS S3.

Example using `du -H`: `/home/faird/shared/code/external/utilities/s5cmd/s5cmd --endpoint-url https://s3.msi.umn.edu du -H s3://MY_BUCKET/*`

Some additional setup is required to use s5cmd:

- run `s5cmd` once with `--install-completion`, and follow the instructions to add the code needed for path completion to your `~/.bashrc`
- export the environment variables `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` -- this can be also be done in your `~/.bashrc`, by adding the following:
```
export AWS_SECRET_ACCESS_KEY=$(s3info | grep Secret | awk '{ print $NF }' )
export AWS_ACCESS_KEY_ID=$(s3info | grep Access | awk '{ print $NF }' )
```

Currently, s5cmd cannot resume interrupted file transfers (upload or download), so it is advisable **not** to use it over s3cmd for file transfers.


For questions, suggestions, or to note any errors, post an issue on our [Github](https://github.com/DCAN-Labs/cdni-brain/issues).
