# S3 / Tier 2

Read: [s3cmd usage page](https://s3tools.org/usage)


S3 is the Tier 2 storage system for storing large datasets. Each PI has 120 TB of bucket space on S3, which is why it is preferred to store data here as opposed to tier 1. For a comprehensive introduction to s3, see [this tutorial video](https://drive.google.com/drive/folders/1Oz3i5lbld5VmXGdhWagOMWYOIJmgrJA4) (recorded as part of the 2022 Data Processing Workshop). It is recommended you watch this tutorial prior to your first time using s3 commands. Note that after you run workbench commands, make sure to `module rm workbench` prior to doing any `s3cmd` commands.


## s3 Basic Commands

1. Setting up the bucket and basic commands:

    - To make a new bucket: `s3cmd mb s3://bucket-name/`

        - **NOTE:** Do not include underscores or uppercase letters in your bucket name. MSI might still let you create a bucket with an uppercase, but it can cause issues with other programs that interact with s3 buckets. As a guideline, follow the example bucket name ("s3://bucket-name/") above. For more information, [visit this link](https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucketnamingrules.html).

    - To list a bucket's contents: `s3cmd ls s3://bucket-name/`

    - To add a file to the bucket: `s3cmd sync path/to/file s3://bucket-name/`
   
    - To add a directory to the bucket: `s3cmd sync --recursive dir_name s3://bucket-name/ `
        - If your job runs out of time before the data upload is complete, use `s3cmd sync --recursive --no-check-md5` to restart the upload process where it left off and assure 
        everything makes it. It's smart to run this command even if the sync says 'Complete' in order to validate. 

        - If trying to upload a lot of data to an s3 bucket (will take >15 min to sync), make sure to first grab an [srun interactive job](slurm_params.md)
          
        - If you're re-running a pipeline/process and you only want the new outputs to be in the directory that you're syncing to, include the `--delete-removed` flag to your sync command. This will remove files that are present in the destination path but aren't present in the source path.

    - To pull down a single file from a bucket: `s3cmd get s3://path/to/file /path/to/where/you/want/file/`

    - To place a single file into a bucket: `s3cmd put path/to/file s3://path/to/where/you/want/file/`

    - To delete a file in a bucket: `s3cmd rm s3://bucket-name/filename` 

    - To delete a bucket: `s3cmd rb s3://bucket-name/ --recursive` 

    - To check the size of a bucket: `s3cmd du -H s3://bucket-name/`

        - This process can take awhile and an srun will most likely be needed. Recommended: use [s5cmd client](#s5cmd-client) for commands such as `du` and `ls` on large buckets

    - To understand how much of your individual s3 quota you have used: `s3info -u x500`

    - For more s3 command usage, see [here](https://s3tools.org/usage).

## Adding/Removing Users from an s3 Bucket  

Permissions to buckets are set with an "S3 policy." , so don't forget to include yourself, especially if it isn't your bucket (you can revoke your own access).

- To be able to edit a policy, this line `export PATH=/home/dhp/public/storage/s3policy_bin/:$PATH` must be in your `~/.bashrc` file. 
    - Open your `~/.bashrc` with a text editor: `textadept ~/.bashrc &`
    - Copy and paste `export PATH=/home/dhp/public/storage/s3policy_bin/:$PATH` into the file and save.
    - For this change to take effect, close your terminal/OpenOnDemand session and open a new terminal.
    - You only need to do this once. 

- Check who already has access to a bucket [here](https://docs.google.com/spreadsheets/d/1QpKYJQqhuxoQhErBscAEev9npsd1RgKS8KdCL6FiuEo/edit?gid=0#gid=0) or by running `/home/faird/shared/code/internal/utilities/MSI-utilities/s3_get_x500/get_x500.sh s3://bucket_name/`
    
    - Please update the [CDNI Data Location spreadsheet](https://docs.google.com/spreadsheets/d/1QpKYJQqhuxoQhErBscAEev9npsd1RgKS8KdCL6FiuEo/edit?gid=0#gid=0) if needed. 

- Update an s3 policy (**DO NOT** include the s3:// prefix in the command): `set_s3policy x500_1,x500_2,etc bucket_name FULL` 

    - **IMPORTANT: every time you set a policy you need to list EVERYONE that needs access. DO NOT just enter that one person's x500 or only that person will have access to the bucket! (Along with the bucket owner)**

20. Instructions to the manual method of setting an s3 policy on MSI can be found at [this link](https://www.msi.umn.edu/support/faq/how-do-i-use-s3-buckets-share-data-tier-2-storage-other-users). An example policy file that gives full access to the s3 bucket  “bucket_name” is as follows:

    ![Example s3 policy](img/s3-policy.png)


## Setting up a .s3cfg

79. Setting up a .s3cfg file: 

    - Open a new terminal in MSI and create a file named `.s3cfg-RANDOM` in your home directory

        * Can't just be `.s3cfg`, name it something identifiable 

    - Run `s3cmd -c ~/.s3cfg-RANDOM --configure` and provide your Access Key ID and Secret Access Key when prompted

    - You shouldn't have to make any changes to the file unless the s3 bucket is hosted by something different than *s3.amazonaws.com*  

    - Test your access through an `ls` command - the `s3cmd -c ~/.s3cfg-RANDOM` will have to be part of every command you run to access the buckets that require the credentials you provided.

## s3 access with Cyberduck 
Resource: [View your MSI S3 credentials (login required)](https://www.msi.umn.edu/content/s3-credentials) 

Read: [Cyberduck S3 documentation](https://docs.cyberduck.io/protocols/s3/)

[Cyberduck](https://cyberduck.io/) is a cloud storage browser for Windows and macOS devices, which can be used to access s3 storage from your local machine. To access MSI Tier 2 storage in Cyberduck, connect to *s3.msi.umn.edu* using your s3 credentials, which can be retrieved from the link at the top of this subsection or by running `s3info` in your MSI terminal. 

To log in for the first time, follow the following steps:

- Click "Open Connection" in the top left corner of the screen

- Select "Amazon s3" from the drop down menu

- Type in *s3.msi.umn.edu* for the server 

- Enter you access and secret keys 

- Click "Connect"

## s5cmd client

Read: [s5cmd GitHub](https://github.com/peak/s5cmd)

The **s5cmd** client is installed on MSI at `/home/faird/shared/code/external/utilities/s5cmd/s5cmd`.

**s5cmd** is an alternative S3 client that performs much faster on MSI than s3cmd for certain operations (roughly 10x-100x faster for `du` and `ls`). Its usage is similar to s3cmd, but does require the additional argument  `--endpoint-url https://s3.msi.umn.edu` so that commands are directed to MSI's S3 (Tier 2) and not AWS S3.

Example using `du -H`: `/home/faird/shared/code/external/utilities/s5cmd/s5cmd --endpoint-url https://s3.msi.umn.edu du -H s3://MY_BUCKET/*`

Some additional setup is required to use s5cmd:

- run `s5cmd` once with `--install-completion`, and follow the instructions to add the code needed for path completion to your `~/.bashrc`
- export the environment variables `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` -- this can be also be done in your `~/.bashrc`, by adding the following:
```
export AWS_SECRET_ACCESS_KEY=$(s3info | grep Secret | awk '{ print $NF }' )
export AWS_ACCESS_KEY_ID=$(s3info | grep Access | awk '{ print $NF }' )
```

Currently, s5cmd cannot resume interrupted file transfers (upload or download), so it is advisable **not** to use it over s3cmd for file transfers.


For questions, suggestions, or to note any errors, post an issue on our [Github](https://github.com/DCAN-Labs/cdni-brain/issues).
